version: "3.8"

services:

  zookeeper:
      image: confluentinc/cp-zookeeper:7.6.0
      environment:
        ZOOKEEPER_CLIENT_PORT: 2181
      ports:
        - "22181:2181"
      volumes:
        - zookeeper-data:/var/lib/zookeeper
      restart: unless-stopped



  kafka:
    image: confluentinc/cp-kafka:7.6.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - ./kafka-data:/var/lib/kafka  # optional
    restart: unless-stopped

  producer:
    build:
      context: .
      dockerfile: Dockerfile.producer
    depends_on:
      - kafka
    volumes:
      - .:/app
    environment:
      - BOOTSTRAP_SERVERS=kafka:29092
    command: >
      watchmedo auto-restart --pattern="*.py" --recursive -- 
      python scraper_producer.py

  consumer:
    build:
      context: .
      dockerfile: Dockerfile.consumer
    depends_on:
      - kafka
    ports:
      - "5001:5000"
    volumes:
      - .:/app
    command: >
      watchmedo auto-restart --pattern="*.py" --recursive -- 
      spark-submit 
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1 
        /app/spark_streaming_consumer.py
    restart: unless-stopped


volumes:
  zookeeper-data:
  kafka-data:
